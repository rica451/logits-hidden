from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

from transformers.models.llama.modeling_llama import LlamaRotaryEmbedding
# from gcld3 import NNetLanguageIdentifier
def is_english(token):
    # identifier = NNetLanguageIdentifier(0, 1000)
    # raw_detection = identifier.FindLanguage(token)
    # if raw_detection.language == "en":
    #     return True
    return False
def show_token(hidden_states ,layer_idx ,token_idx ,model ,top_k=10):

    hidden = hidden_states[layer_idx][0, token_idx, :].unsqueeze(0)
    normalized_hidden = model.model.norm(hidden)
            
    logits = model.lm_head(normalized_hidden)
    topk = torch.topk(logits, k=top_k)
            
    # ä¿®æ­£å€™é€‰tokençš„æ˜¾ç¤º
    # decoded_tokens = correct_chinese_bytes(
    #     tokenizer.convert_ids_to_tokens(topk.indices[0])
    # )
    # preds = " | ".join([f"{t} ({s:.2f})" for t, s in zip(decoded_tokens, topk.values[0].tolist())])
    token_ids = topk.indices[0].tolist()
    preds = " | ".join([f"{tid} ({s:.2f})" for tid, s in zip(token_ids, topk.values[0].tolist())])
    print(f"Layer {layer_idx:2d}: {preds}")

def decode_hidden_to_tokens(model, inputs, modified_hidden, layer_idx):
    with torch.no_grad():
        hidden = model.model.embed_tokens(inputs["input_ids"])

        # # ä»LlamaModelè·å–å…±äº«çš„rotary_emb
        rotary_emb = model.model.rotary_emb
        
        # # å‰å‘ä¼ æ’­åˆ°ç›®æ ‡å±‚ä¹‹å‰çš„å±‚
        for i, layer in enumerate(model.model.layers[:layer_idx]):
            seq_len = hidden.shape[1]
            position_ids = torch.arange(seq_len, device=hidden.device).unsqueeze(0)
            
            # ç”Ÿæˆcos/sin
            cos, sin = rotary_emb(hidden, position_ids)
            
            # è°ƒç”¨decoder layeræ—¶éœ€è¦ä¼ é€’position_embeddings
            hidden = layer(
                hidden,
                position_embeddings=(cos, sin),
                attention_mask=inputs["attention_mask"].to(hidden.dtype),
                position_ids=position_ids,
                use_cache=False
            )[0]

        # åº”ç”¨ä¿®æ”¹åçš„hidden_states
        hidden = modified_hidden
        new_seq_len = hidden.shape[1]
        
        for layer in model.model.layers[layer_idx:]:
            # ç”Ÿæˆæ–°çš„ä½ç½®ç¼–ç 
            position_ids = torch.arange(new_seq_len, device=hidden.device).unsqueeze(0)
            cos, sin = rotary_emb(hidden, position_ids)
            
            attention_mask = inputs["attention_mask"][:, :new_seq_len] if inputs["attention_mask"] is not None else None
            
            hidden = layer(
                hidden,
                position_embeddings=(cos, sin),
                attention_mask=attention_mask.to(hidden.dtype),
                position_ids=position_ids,
                use_cache=False
            )[0]

        hidden = model.model.norm(hidden)
        logits = model.lm_head(hidden)
    return logits



# def filter_english_in_hidden(model, hidden_states, layer_idx, tokenizer):
#     modifiled_hidden = hidden_states.clone()
#     seq_len = hidden_states.shape[1]
#     banned_token_ids = [9093, 1234, 5678]
#     V = model.lm_head.weight[banned_token_ids]  # [k, hidden_dim]
#     V_float32 = V.float()
#     V = torch.nn.functional.normalize(V, dim=-1)
#     VVt = V_float32 @ V_float32.T
#     VVt_inv = torch.linalg.pinv(VVt)
#     P_float32 = V_float32.T @ VVt_inv @ V_float32
#     P = P_float32.to(hidden_states.dtype)
#     # æ­£äº¤æŠ•å½±çŸ©é˜µ P = V^T (V V^T)^-1 V
#     # P = V.T @ torch.inverse(V @ V.T) @ V  # [hidden_dim, hidden_dim]

#     # æŠ•å½± hidden åˆ°å±è”½å­ç©ºé—´
#     projection = hidden_states @ P  # [batch, seq_len, hidden_dim]
#     modifiled_hidden = hidden_states - projection
#     # for pos in range(seq_len):
#     #     hidden_vector = hidden_states[0, pos, :].unsqueeze(0)
#     #     normalized = model.model.norm(hidden_vector)
#     #     logits = model.lm_head(normalized)
#     #     token_id = torch.argmax(logits, dim=-1).item()

#     #     token = tokenizer.decode([token_id])
#     #     if is_english(token):
#     #         print(f"Filtering English token at layer {layer_idx} position {pos}: {token}")
#     #         modifiled_hidden[0, pos, :] = 0
#     return modifiled_hidden

def filter_english_in_hidden(model, hidden_states, layer_idx, tokenizer, topk=5):
    modifiled_hidden = hidden_states.clone()
    seq_len = hidden_states.shape[1]
    aklÄ±_id = tokenizer.encode("esimal", add_special_tokens=False)[0]
    banned_token_ids = [aklÄ±_id, 27946, 1097, 108504, 29061, 58731, 42888,69952, 56405, 23881,42888, 117283, 69952, 56405, 23881,84555, 85869, 80979, 63524,7126, 6699, 1027, 47797, 19144]
    Î» = 1
    V = model.lm_head.weight[banned_token_ids]
    V_float32 = V.float()
    V = torch.nn.functional.normalize(V, dim=-1)
    VVt = V_float32 @ V_float32.T
    VVt_inv = torch.linalg.pinv(VVt)
    P_float32 = V_float32.T @ VVt_inv @ V_float32
    P = P_float32.to(hidden_states.dtype)
    

    # è®¡ç®— projection å¹¶å¾—åˆ°ä¿®æ”¹åçš„éšè—çŠ¶æ€
    projection = hidden_states @ P
    modifiled_hidden = hidden_states - Î» * projection

    print(f"\n===== Layer {layer_idx} Top-{topk} Token Changes =====")
    for pos in range(seq_len):
        orig_hidden = hidden_states[0, pos, :].unsqueeze(0)  # [1, hidden_dim]
        mod_hidden = modifiled_hidden[0, pos, :].unsqueeze(0)

        # æ­£å¸¸åŒ–ï¼ˆå¦‚æœæ¨¡å‹æœ‰ norm å±‚ï¼‰
        if hasattr(model.model, 'norm'):
            orig_hidden = model.model.norm(orig_hidden)
            mod_hidden = model.model.norm(mod_hidden)

        orig_logits = model.lm_head(orig_hidden)  # [1, vocab_size]
        mod_logits = model.lm_head(mod_hidden)

        orig_topk = torch.topk(orig_logits, topk, dim=-1)
        mod_topk = torch.topk(mod_logits, topk, dim=-1)

        orig_ids = orig_topk.indices[0].tolist()
        mod_ids = mod_topk.indices[0].tolist()
        orig_tokens = [tokenizer.decode([tid]) for tid in orig_ids]
        mod_tokens = [tokenizer.decode([tid]) for tid in mod_ids]

        if orig_ids != mod_ids:
            print(f"[Position {pos}]")
            print(f"  Before: {orig_tokens}")
            print(f"  After : {mod_tokens}")
    aklÄ±_id = tokenizer.encode("aklÄ±", add_special_tokens=False)[0]
    aklÄ±_vector = model.lm_head.weight[aklÄ±_id]  # [hidden_dim]

    # æŠ•å½±åˆ†é‡å¤§å°
    proj = aklÄ±_vector @ P  # [hidden_dim]
    proj_norm = torch.norm(proj)
    aklÄ±_norm = torch.norm(aklÄ±_vector)
    cosine_proj = proj_norm / aklÄ±_norm
    print(f"aklÄ±'s vector projection magnitude: {cosine_proj:.4f}")
    return modifiled_hidden

def main():
    model_name = "meta-llama/Llama-3.1-8B-Instruct"
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        output_hidden_states=True,
        trust_remote_code=True
    )
    model.eval()
    text = "è¯·ä½ è·Ÿç€æˆ‘æ•°ï¼Œ0ï¼Œ1ï¼Œ2ï¼Œ3ï¼Œ4"
    # text = "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çˆ¸çˆ¸ï¼Œè¶…å¼Ÿçš„åå­—å«ç”Ÿå‘½"
    inputs = tokenizer(text, return_tensors="pt").to(model.device)

    inputs["position_ids"] = torch.arange(
        inputs["input_ids"].shape[1], device=inputs["input_ids"].device
    ).unsqueeze(0)
    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)
        logits = outputs.logits

        predicted_ids = torch.argmax(logits, dim=-1)[0]  
        predicted_tokens = [tokenizer.decode([token_id], skip_special_tokens=True) for token_id in predicted_ids.tolist()]

        print("ğŸ“ Model Predicted Tokens:")
        for i, token in enumerate(predicted_tokens):
            print(f"{i+1}. {token}")
        print("ğŸ“ Model Predicted Text:")
        print(predicted_tokens)

        hidden_states = outputs.hidden_states[20]

    modified_hidden = filter_english_in_hidden(model, hidden_states, 20, tokenizer)
    logits = decode_hidden_to_tokens(model, inputs, modified_hidden, 20)

    final_tokens = torch.argmax(logits, dim=-1)[0]
    final_tokens = tokenizer.decode(final_tokens.tolist())
    
    print(f"Final tokens: {final_tokens}")

if __name__ == "__main__":
    main()